{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAN-4twhxtat"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1M_V9Byxta0"
      },
      "source": [
        "<h2> Basic function </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0no3fwWyxta4"
      },
      "outputs": [],
      "source": [
        "# Split data into train_data and valid_data\n",
        "def split_data(x, y, vaild_ratio=0.2, random_state=None):\n",
        "    data_size = x.shape[0]\n",
        "    split_id = int(data_size * vaild_ratio)\n",
        "\n",
        "    index = np.arange(data_size)\n",
        "    np.random.shuffle(index)\n",
        "    x = x[index]\n",
        "    y = y[index]\n",
        "\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    train_x, valid_x = x[split_id:], x[:split_id]\n",
        "    train_y, valid_y = y[split_id:], y[:split_id]\n",
        "\n",
        "    return train_x, valid_x, train_y, valid_y\n",
        "\n",
        "def rmse(true_y, pred_y):\n",
        "        return np.sqrt(np.mean((true_y - pred_y)**2))\n",
        "\n",
        "def fac(n):\n",
        "    if (n == 0 or n == 1):\n",
        "        return np.array([1])\n",
        "    else:\n",
        "        return np.array([n * fac(n-1)])\n",
        "        \n",
        "def product(*args, repeat=1): # Cartesian product\n",
        "        pools = [tuple(pool) for pool in args] * repeat\n",
        "        result = [[]]\n",
        "        for pool in pools:\n",
        "            result = [x+[y] for x in result for y in pool]\n",
        "        for prod in result:\n",
        "            yield tuple(prod)\n",
        "\n",
        "def replacement(iterable, r): # when n > 0, return the number of (n + r - 1)! / r! / (n - 1)!\n",
        "        pool = tuple(iterable)\n",
        "        n = len(pool)\n",
        "        for indices in product(range(n), repeat=r):\n",
        "            if sorted(indices) == list(indices):\n",
        "                yield tuple(pool[i] for i in indices)\n",
        "\n",
        "def standard_norm(x):\n",
        "    mean = np.mean(x, axis=0)\n",
        "    std = np.std(x, axis=0)\n",
        "    return (x - mean) / std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvHATPqexta6"
      },
      "source": [
        "<h1>2 Linear Regression</h1>\n",
        "<h3>2.1 Feature Selection</h3>\n",
        "In real-world applications, the dimension of data is usually more than one. In the training\n",
        "stage, please fit the data by applying a polynomial function of the form\n",
        "\n",
        "$$\n",
        "    y = (\\textbf{x}, \\textbf{w}) = w_0 + \\sum_{i=1}^D w_i x_i + \\sum_{i=1}^D\\sum_{j=1}^D w_ix_ix_j \\ (M = 2)\n",
        "$$\n",
        "\n",
        "and minimizing the error function\n",
        "\n",
        "$$\n",
        "    E(\\textbf{w}) = \\sqrt{\\frac{1}{N} \\sum_{n=1}^N \\{y(x_n, \\textbf{w}) - t_n\\}^2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGX173Jpxta8"
      },
      "source": [
        "(a) In the feature selection stage, please apply polynomials of order $M = 1$ and $M = 2$\n",
        "\n",
        "over the input data with dimension $D = 11$. Please evaluate the corresponding RMS error on the training set and valid set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p5S_n3jxta-"
      },
      "source": [
        "Root Mean Square Error\n",
        "<p align=\"center\">\n",
        "    <img src=\"image/rmse.png\"/>\n",
        "</p>\n",
        "Least Square Solution\n",
        "<p align=\"center\">\n",
        "    <img src=\"image/lsq.jpg\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug3d8tifxta_"
      },
      "outputs": [],
      "source": [
        "# Implement details from sklearn LinearRegression model\n",
        "class LinearRegression:\n",
        "    # hyper is for regularization\n",
        "    def __init__(self, learning_rate=0.0001, iter=1000, hyper=0):\n",
        "        self.hyper = hyper\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iter = iter\n",
        "        self.random_rate = 1e-7\n",
        "        self.weight = None\n",
        "        self.bias = None\n",
        "        \n",
        "    def fit(self, x, y):\n",
        "        self.data_size, self.feature_number = x.shape\n",
        "\n",
        "        # Using least square solution to find the best weight and bias\n",
        "        # We need to add a matrix to prevent generate singular matrix, making it can be inversed correctly\n",
        "        # If we use hyper parameter, it will turn into be MAP, otherwise is MLE\n",
        "        bias_inputs = np.ones((self.data_size, 1))\n",
        "        phi = np.concatenate((bias_inputs, x), axis=1)\n",
        "        noise = self.hyper * np.eye(self.feature_number+1)\n",
        "        normal = np.dot(phi.T, phi) + self.random_rate * np.random.randn(self.feature_number+1)\n",
        "        inverse_equ = np.linalg.inv(normal + noise)\n",
        "        theta = np.dot(inverse_equ, np.dot(phi.T, y))\n",
        "        self.bias, self.weight = theta[0], theta[1:]\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.dot(x, self.weight) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjHNTdkGxtbD"
      },
      "outputs": [],
      "source": [
        "data_x_df = pd.read_csv('X.csv')\n",
        "data_t_df = pd.read_csv('T.csv')\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = split_data(data_x_df.values, data_t_df.values, vaild_ratio=0.2, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKmEV8YGxtbE",
        "outputId": "b5f263ea-4f77-419b-cb15-3a8024552b87"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "model.fit(train_x, train_y)\n",
        "\n",
        "# RMS error\n",
        "print(\"M = 1\")\n",
        "predict_y = model.predict(train_x)\n",
        "print('Train RMS error: {}'.format(rmse(train_y, predict_y)))\n",
        "baseline = rmse(train_y, predict_y)\n",
        "predict_y2 = model.predict(valid_x)\n",
        "print('Test RMS error: {}'.format(rmse(valid_y, predict_y2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img src=\"image/Output_5.png\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mO6q7qgxtbF"
      },
      "outputs": [],
      "source": [
        "# We can use polynomial features to define polynomial function\n",
        "# It's implemented from sklearn PolynomialFeatures\n",
        "class PolynomialFeatures:\n",
        "\n",
        "    def __init__(self, x, degree=2):\n",
        "        self.data = x\n",
        "        self.degree = degree\n",
        "        self.data_size = x.shape[0]\n",
        "        self.feature_num = x.shape[1]\n",
        "        self.numerator = fac(self.feature_num + self.degree) # (11 + 2)!\n",
        "        self.denominator = fac(self.degree) * fac(self.feature_num) # 11! * 2!\n",
        "\n",
        "    def fit(self):\n",
        "        # calculate number of output feature size\n",
        "        self.n_output_features = int(self.numerator / self.denominator) - 1 # (13 * 12 / 2) - 1 = 77\n",
        "    \n",
        "    def transform(self):\n",
        "        # Transform data to polynomial features.\n",
        "        # feature_tuple is list of tuples indices to calculate polynomial features\n",
        "        # Create list of tuples containing feature index combinations.\n",
        "        # to store new array from transformation\n",
        "        feature_tuple = [replacement(range(self.feature_num), idx)\n",
        "                for idx in range(1, self.degree+1)]\n",
        "        combinations = [item for sublist in feature_tuple for item in sublist]\n",
        "        x_new = np.empty((self.data_size, self.n_output_features))\n",
        "\n",
        "        for i, index_feature_tuple in enumerate(combinations):\n",
        "            x_new[:, i] = np.prod(self.data[:, index_feature_tuple], axis=1)\n",
        "\n",
        "        return x_new\n",
        "    \n",
        "    \n",
        "    def predict(self, x):\n",
        "        return np.dot(x, self.weight) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gghfa--ZxtbH",
        "outputId": "089b7b26-8c87-4d59-c5e5-51ac57a0fe99"
      },
      "outputs": [],
      "source": [
        "transform = PolynomialFeatures(data_x_df.values, degree=2)\n",
        "transform.fit()\n",
        "x_2 = transform.transform()\n",
        "train_x, valid_x, train_y, valid_y = split_data(x_2, data_t_df.values, vaild_ratio=0.2, random_state=1)\n",
        "\n",
        "model2 = LinearRegression()\n",
        "model2.fit(train_x, train_y)\n",
        "\n",
        "# RMS error\n",
        "print(\"M = 2\")\n",
        "y_pred = model2.predict(train_x)\n",
        "print('Train RMS error: {}'.format(rmse(train_y, y_pred)))\n",
        "y_pred = model2.predict(valid_x)\n",
        "print('Test RMS error: {}'.format(rmse(valid_y, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img src=\"image/Output_7.png\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ycROhaSxtbI"
      },
      "source": [
        "(b) How will you analysis the weights of polynomial model $M = 1$ and select the most contributive feature?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt93PLW6xtbI",
        "outputId": "6de5b3ca-386c-4a13-b602-b2c26df85ae6"
      },
      "outputs": [],
      "source": [
        "row = 0\n",
        "for i in data_x_df.columns:\n",
        "    print(i, end=\": \")\n",
        "    for j in model.weight[row:row+1]:\n",
        "        print(j)\n",
        "    row += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img src=\"image/Output_8.png\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIwYl8zgxtbJ"
      },
      "source": [
        "Correlation for each feature in x data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8OkDZx8xtbK",
        "outputId": "32544a35-a013-4e02-b0b1-8f86e2a1a074"
      },
      "outputs": [],
      "source": [
        "arr_x = np.array(data_x_df)\n",
        "arr_y = np.array(data_t_df)\n",
        "for i in range(11):\n",
        "    print(data_x_df.columns[i])\n",
        "    print(np.corrcoef(arr_x[:, i], arr_y[:,0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img src=\"image/Output_9.png\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cglRkFF7xtbM"
      },
      "source": [
        "<style>\n",
        "\n",
        ".red{\n",
        "    color: red;\n",
        "};\n",
        "\n",
        "</style>\n",
        "\n",
        "When M = 1, the most positive value of the coefficient is sulphates.\n",
        "\n",
        "But as we use corrcoef function, the most contributive is **alcohol**.\n",
        "\n",
        "<span class=\"red\">So I think, **alcohol** is the most contributive feature.</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmx4XSHyxtbN"
      },
      "source": [
        "<h3>2.2 Maximum likelihood approach</h3>\n",
        "\n",
        "(a) Which basis function will you use to further improve your regression model, polynomial, Gaussian, Sigmoid, or hybrid?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG7qDQNFxtbN"
      },
      "source": [
        "There are three choices.\n",
        "\n",
        "Firstly, **polynomial basis** $y(x,w) = \\sum_{j=0}^{M-1} w_j \\phi_j(x) = w^T \\phi(x)$\n",
        "\n",
        "If we choose it, we may need to face some problems\n",
        "\n",
        "1. It's diccicult to formulate\n",
        "2. It needs to use different polynomials in each region\n",
        "3. Polynomials are global basis functions, each affecting the prediction over the whole input space\n",
        "\n",
        "For the **Gaussian Radial Basis Functions** $\\phi_j (x) = \\rm{exp}(\\frac{(x - \\mu)^2}{2 \\sigma^2})$\n",
        "\n",
        "- $\\mu$ govern the locations of the basis functions\n",
        "- $\\sigma$ governs the spatial scale\n",
        "- [Demo graph from desmos](https://www.desmos.com/calculator/uc1hqmb09u?lang=zh-TW)\n",
        "\n",
        "The last one, Sigmoidal Basis Function $\\phi_j (x) = \\sigma(\\frac{x -\\mu_j}{s})$ where $\\sigma(a) = \\frac{1}{1 + \\rm{exp}(-a)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BuG58B3xtbO"
      },
      "source": [
        "<style>\n",
        "\n",
        ".red{\n",
        "    color: red;\n",
        "};\n",
        "\n",
        "</style>\n",
        "\n",
        "After above discussion, <span class=\"red\">I will choose to use sigmoid basis function.</style>\n",
        "\n",
        "Since it can be combined to create a model called **Artifical Neueal Network (ANN)**\n",
        "\n",
        "In this situation, we will normalize input data, and than transfer it with sigmoid basis function.\n",
        "\n",
        "Such that we can use the preprocessing data to analysis with Linear Regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKGmA8s_xtbP"
      },
      "source": [
        "(b) Introduce the basis function you just decided in (a) to linear regression model and analyze the result you get.\n",
        "\n",
        "$$\n",
        "    \\phi(x) = [\\phi_1(x), \\phi_2(x), ..., \\phi_N(x), \\phi_{bias}(x)]\n",
        "$$\n",
        "<p align=\"center\">\n",
        "    <img src=\"image/2.2.a.png\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"image/standard.jpg\"/>   \n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TtKDEdmxtbP",
        "outputId": "865e8d9c-414d-40b5-b174-9ea95770b84b"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(- (x - np.mean(x, axis=0)) / np.std(x, axis=0)))\n",
        "\n",
        "norm_x = standard_norm(data_x_df.values)\n",
        "\n",
        "basis_x = sigmoid(norm_x)\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = split_data(basis_x, data_t_df.values, vaild_ratio=0.2, random_state=1)\n",
        "\n",
        "model_M = LinearRegression()\n",
        "model_M.fit(train_x, train_y)\n",
        "\n",
        "# RMS error\n",
        "train_pred = model_M.predict(train_x)\n",
        "test_pred = model_M.predict(valid_x)\n",
        "\n",
        "print('Train RMS error: {}, Test RMS error: {}'.format(rmse(train_y, train_pred), rmse(valid_y, test_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img src=\"image/Output_10.png\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWEMPzeGxtbQ"
      },
      "source": [
        "(c) Apply N -fold cross-validation in your training stage to select at least one hyperparameter (order, parameter number, . . .) \n",
        "\n",
        "for model and do some discussion (underfitting, overfitting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kLyjrfqxtbR",
        "outputId": "1e9d53e8-169c-4a11-a6dd-c54530a23786"
      },
      "outputs": [],
      "source": [
        "# Implement details from sklearn model KFold\n",
        "class KFold:\n",
        "    def __init__(self, n_splits=3):\n",
        "        self.now = 0\n",
        "        self.one = 1\n",
        "        \n",
        "        if n_splits <= 1:\n",
        "            raise ValueError(\n",
        "                \"k-fold cross-validation requires at least one\"\n",
        "                \" train/test split by setting n_splits=2 or more,\"\n",
        "                \" got n_splits={0}.\".format(n_splits)\n",
        "            )\n",
        "\n",
        "        self.n_splits = n_splits\n",
        "\n",
        "    def split(self, X):\n",
        "        self.indices = np.arange(len(X))\n",
        "        train_list = []\n",
        "        test_list = []\n",
        "\n",
        "        np.random.RandomState(self.one).shuffle(self.indices)\n",
        "\n",
        "        for id in self.mask(X):\n",
        "            train_list.append(self.indices[np.logical_not(id)])\n",
        "            test_list.append(self.indices[(id)])\n",
        "\n",
        "        return train_list, test_list\n",
        "        \n",
        "    def mask(self, X): # Generates boolean masks corresponding to test sets.\n",
        "        mask_list = []\n",
        "        for id in self.indice(X): \n",
        "            mask = np.zeros(len(X), dtype=bool)\n",
        "            mask[id] = True\n",
        "            mask_list.append(mask)\n",
        "\n",
        "        return mask_list\n",
        "    \n",
        "    def indice(self, X): # Generates integer indices corresponding to test sets.\n",
        "        n_splits = self.n_splits\n",
        "        fold_sizes = np.full(n_splits, len(X)//n_splits, dtype=int)\n",
        "        fold_sizes[:len(X) % n_splits] += 1\n",
        "        index = []\n",
        "        for fold_size in fold_sizes:\n",
        "            start, stop = self.now, self.now + fold_size\n",
        "            index.append(self.indices[start:stop])\n",
        "            self.now = stop\n",
        "        \n",
        "        return index\n",
        "\n",
        "kf = KFold(n_splits=3)\n",
        "training_indices, indice = kf.split(basis_x)\n",
        "\n",
        "train_rmse = np.zeros(len(data_t_df))\n",
        "test_rmse = np.zeros(len(data_t_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for m in range(1, 7):\n",
        "    train_rmse = np.zeros(len(data_t_df))\n",
        "    test_rmse = np.zeros(len(data_t_df))\n",
        "    \n",
        "    for train_idx, index in zip(training_indices, indice):\n",
        "        transform = PolynomialFeatures(basis_x, degree=m)\n",
        "        transform.fit()\n",
        "        polynomial_x = transform.transform()\n",
        "        train_x, test_x = polynomial_x[train_idx], polynomial_x[index]\n",
        "        train_y, test_y = data_t_df.values[train_idx], data_t_df.values[index]\n",
        "\n",
        "        model_polynomial = LinearRegression()\n",
        "        model_polynomial.fit(train_x, train_y)\n",
        "\n",
        "        # RMS error\n",
        "        train_pred = np.array(model_polynomial.predict(train_x))\n",
        "        test_pred = np.array(model_polynomial.predict(test_x))\n",
        "        train_rmse = np.append(train_rmse, rmse(train_y, train_pred))\n",
        "        test_rmse = np.append(test_rmse, rmse(test_y, test_pred))\n",
        "    \n",
        "    average_train_rmse = sum(train_rmse) / len(training_indices)\n",
        "    average_test_rmse = sum(test_rmse) / len(training_indices)\n",
        "    print('M = {}, Train RMS error: {}, Test RMS error: {}'.format(m, average_train_rmse, average_test_rmse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img src=\"image/Output_11.png\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X074BbjyxtbS"
      },
      "source": [
        "<h3> 2.3 Maximum a posteriori approach </h3>\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"image/2.3.png\"/>\n",
        "</p>\n",
        "\n",
        "(a) What is the key difference between maximum likelihood approach and maximum a posteriori approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqPy9ou9xtbS"
      },
      "source": [
        "Maximum posteriori approach (MAP) affect from priori proability $p (\\theta \\ | \\ m)$, \n",
        "\n",
        "but maximum likelihood approach (MLE) doesn't affect by it.\n",
        "\n",
        "Priori proability, which means the probability based on past experience and analysis.\n",
        "\n",
        "MLE, comparing with MAP. It's much easier to get overfit. \n",
        "\n",
        "Because MAP will use priori proability's and experimential data's info. to predict the testing data.\n",
        "\n",
        "The influence determines by their weight, which is set by model designer.\n",
        "\n",
        "---\n",
        "\n",
        "MAP: \n",
        "\n",
        "$$ \\ p(\\theta \\ | \\ \\mathcal{D}, m) = \\frac{p(\\mathcal{D} \\ | \\ m, \\theta) p(\\theta \\ | \\ m)}{p(\\mathcal{D} \\ | \\ m)} $$\n",
        "\n",
        "MLE:\n",
        "\n",
        "$$p(\\mathcal{D} \\ | \\ m, \\theta)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23T_nkNjxtbT"
      },
      "source": [
        "(b) Use maximum a posteriori approach method to retest the model in 2.2 you designed.\n",
        "\n",
        "You could choose Gaussian distribution as a prior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6sTHDH2xtbT",
        "outputId": "82df2d7d-14d6-4b10-a07b-3df472d1d13a"
      },
      "outputs": [],
      "source": [
        "for m in range(1, 7):\n",
        "    transform = PolynomialFeatures(basis_x, degree=m)\n",
        "    transform.fit()\n",
        "    polynomial_x = transform.transform()\n",
        "    train_x, test_x = polynomial_x[train_idx], polynomial_x[index]\n",
        "    train_y, test_y = data_t_df.values[train_idx], data_t_df.values[index]\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(train_x, train_y)\n",
        "    model_regularization = LinearRegression(hyper=0.01)\n",
        "    model_regularization.fit(train_x, train_y)\n",
        "\n",
        "    # RMS error\n",
        "    train_pred = model.predict(train_x)\n",
        "    test_pred = model.predict(test_x)\n",
        "    trainx_regular = model_regularization.predict(train_x)\n",
        "    testx_regular = model_regularization.predict(test_x)\n",
        "    \n",
        "    print(\"If we don't use priori probability: \")\n",
        "    print('M = {}, Train RMS error: {}, Test RMS error: {}'.format(m, rmse(train_y, train_pred), rmse(test_y, test_pred)))\n",
        "    print(\"If we use priori probability: \")\n",
        "    print('M = {}, Train RMS error: {}, Test RMS error: {}'.format(m, rmse(train_y, trainx_regular), rmse(test_y, testx_regular)))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img src=\"image/Output_12.png\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HvhXEd1xtbU",
        "outputId": "d7ce76c2-798d-48ca-e5d3-ff97b0879aba"
      },
      "outputs": [],
      "source": [
        "row = 0\n",
        "for i in data_x_df.columns:\n",
        "    print(i, end=\": \")\n",
        "    for j in model2.weight[row:row+1]:\n",
        "        print(j)\n",
        "    row += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img src=\"image/Output_13.png\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWwKUrDrxtbU"
      },
      "source": [
        "(c) Compare the result between maximum likelihood approach and maximum a posteriori approach. \n",
        "\n",
        "Is it consistent with your conclusion in (a)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8-WE1LdxtbU"
      },
      "source": [
        "Yes, it's consistent.\n",
        "\n",
        "As we can see the result from previous code's output.\n",
        "\n",
        "**Using MAP can effictively prevent overfitting.** "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
