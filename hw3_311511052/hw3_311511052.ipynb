{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "# you can choose one of the following package for image reading/processing\n",
    "\n",
    "import cv2\n",
    "import PIL\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Support Vector Machine\n",
    "\n",
    "<style>\n",
    ".blue{\n",
    "    color: skyblue;\n",
    "}\n",
    ".bold{\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "In the training procedure of SVM, we need to optimize with respect to the Lagrange multipliers\n",
    "$a = \\{ a_n \\}$. \n",
    "\n",
    "Here, we use the <span class=\"blue\">sequential minimal optimization</span> to solve the problem. \n",
    "\n",
    "For details, you can refer to the paper [Platt, John. “Sequential minimal optimization: \n",
    "\n",
    "A fast algorithm for training support vector machines”, 1998]. The classifier is written by\n",
    "\n",
    "$$\n",
    "    y(\\textbf{x}) = \\sum_{n=1}^N a_n t_n k(\\textbf{x}, \\textbf{x}_n) = \\textbf{w}^T \\textbf{x} + b\n",
    "$$\n",
    "$$\n",
    "    \\textbf{w} = \\sum_{n=1}^N \\alpha_n t_n \\phi(\\textbf{x}_n)\n",
    "$$\n",
    "$$\n",
    "    b = \\frac{1}{N_{\\mathcal{M}}} \\sum_{n \\in \\mathcal{M}} \\left( t_n - \\sum_{m \\in \\mathcal{S}} a_m t_m k(\\textbf{x}_n, \\textbf{x}_m) \\right)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{M}$ denotes the set of indices of data points having $0 \\lt a_n \\lt C$.\n",
    "\n",
    "### 1.1\n",
    "\n",
    "It is popular to use principal component analysis (PCA) to reduce the dimension of images to d = 2. \n",
    "\n",
    "Please implement it by yourself instead of using the method from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./x_train.csv\",header= None)/255\n",
    "label = pd.read_csv(\"./t_train.csv\",header= None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA**\n",
    "\n",
    "$\\textbf{C}$ is covariance matrix of $\\textbf{X}$, $\\textbf{W}$ is the eigenvector matrix of $\\textbf{C}$, $\\Lambda$ is the eigenvalue matrix of $\\textbf{C}$.\n",
    "\n",
    "$\\textbf{X}$ is the data matrix, $\\textbf{X}_k$ is the data matrix after dimension reduction, and k in this task will be 2.\n",
    "\n",
    "$$\n",
    "    \\textbf{C} = \\frac{\\textbf{X}^T \\textbf{X}}{n - 1}\n",
    "$$\n",
    "$$\n",
    "    \\textbf{C} = \\textbf{W} \\Lambda \\textbf{W}^{-1}\n",
    "$$\n",
    "$$\n",
    "    \\textbf{X}_k = \\textbf{X} \\textbf{W}_k\n",
    "$$\n",
    "\n",
    "**Using SVD to solve PCA**\n",
    "\n",
    "$\\Sigma$ is the singular value matrix of $\\textbf{X}$, $\\textbf{U}$ is the left singular vector matrix of $\\textbf{X}$, $\\textbf{V}$ is the right singular vector matrix of $\\textbf{X}$.\n",
    "\n",
    "$$\n",
    "    \\textbf{C} = \\frac{\\textbf{X}^T \\textbf{X}}{n - 1} \n",
    "$$\n",
    "$$\n",
    "    \\textbf{C} = \\frac{\\textbf{V}\\Sigma\\textbf{U}^T\\textbf{U}\\Sigma\\textbf{V}^T}{n - 1}\n",
    "$$\n",
    "$$\n",
    "    \\textbf{C} = \\textbf{V} \\frac{\\Sigma^2}{n - 1}\\textbf{V}^T\n",
    "$$\n",
    "$$\n",
    "    \\textbf{C} = \\textbf{V} \\frac{\\Sigma^2}{n - 1}\\textbf{V}^{-1} \\ \\ (\\rm{because} \\ \\textbf{V} \\ \\rm{is \\ unitary})\n",
    "$$\n",
    "\n",
    "We can found that covariance matrix $\\textbf{C}$ is the same as the eigenvalue matrix $\\Lambda$ of $\\textbf{V}^{-1}$.\n",
    "\n",
    "$$\n",
    "    \\Lambda = \\frac{\\Sigma^2}{n - 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data.values - np.mean(data.values, axis=0) # centralize data\n",
    "np.allclose(data_new.mean(axis=0), np.zeros(data_new.shape[1])) # check if centralization is correct\n",
    "C = (data_new.T @ data_new) / (data_new.shape[0] - 1) # covariance matrix\n",
    "eig_vals, eig_vecs = np.linalg.eig(C) # eigenvalues and eigenvectors\n",
    "data_pca = data_new @ eig_vecs # PCA\n",
    "U, Sigma, V = np.linalg.svd(data_new, full_matrices=False, compute_uv=True) # SVD\n",
    "data_pca_svd = data_new @ V.T # PCA by SVD\n",
    "data_pca_svd = data_pca_svd[:,0:2] # reduce dimension to 2\n",
    "data_pca_svd # 2D data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2\n",
    "\n",
    "Describe the difference between two decision approaches (one-versus-the-rest and one-\n",
    "versus-one). \n",
    "\n",
    "Decide which one you want to choose and explain why you choose this approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-versus-the-rest**\n",
    "\n",
    "**One-versus-one**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3\n",
    "\n",
    "<style>\n",
    ".blue{\n",
    "    color: skyblue;\n",
    "}\n",
    ".bold{\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "Use the principle values projected to top <span class=\"blue\">two</span> eigenvectors obtained from PCA, and build\n",
    "a SVM with <span class=\"blue\">linear kernel</span> to do multi-class classification. \n",
    "\n",
    "You can decide the upper bound $C$ of $a_n$ by yourself or just use the default value provided by sklearn. \n",
    "\n",
    "Then, <span class=\"blue\">plot the corresponding decision boundary</span> and show the <span class=\"blue\">support vectors.</span>\n",
    "\n",
    "* Linear kernel:\n",
    "$$\n",
    "    k(\\textbf{x}_i, \\textbf{x}_j) = \\phi(\\textbf{x}_i)^T \\phi(\\textbf{x}_j) = \\textbf{x}_i^T \\textbf{x}_j\n",
    "$$\n",
    "\n",
    "The sample figures are provided below.\n",
    "\n",
    "<center>\n",
    "    <img src = \"./image/figure1.png\" width = 50%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus\n",
    "\n",
    "<style>\n",
    ".blue{\n",
    "    color: skyblue;\n",
    "}\n",
    ".bold{\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "Repeat 3 with <span class=\"blue\">polynomial kernel (degree = 2).</span>\n",
    "\n",
    "* Polynomial (homogeneous) kernel of degree 2:\n",
    "\n",
    "$$\n",
    "    k(\\textbf{x}_i, \\textbf{x}_j) = \\phi(\\textbf{x}_i)^T \\phi(\\textbf{x}_j) = \\left( \\textbf{x}_i^T \\textbf{x}_j\\right)^2\n",
    "$$\n",
    "$$\n",
    "    \\phi(\\textbf{x}) = \\left[ \\textbf{x}_1^2, \\sqrt{2} \\textbf{x}_1 \\textbf{x}_2, \\textbf{x}_2^2 \\right]\n",
    "$$\n",
    "$$\n",
    "    \\textbf{x} = \\left[ x_1, x_2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Gaussian Mixture Model\n",
    "\n",
    "<style>\n",
    ".blue{\n",
    "    color: skyblue;\n",
    "}\n",
    ".red{\n",
    "    color: red;\n",
    "}\n",
    ".bold{\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "In this exercise, you will implement a Gaussian mixture model (GMM) and apply it in image segmentation. \n",
    "\n",
    "First, use a $K$-means algorithm to find $K$ central pixels. \n",
    "\n",
    "Second, use the expectation maximization (EM) algorithm <span class=\"blue\">(please refer to textbook p.438-p.439)</span> to optimize the parameters of the model. \n",
    "\n",
    "The input image is given by <span class=\"red\">hw3.jpg</span>. \n",
    "\n",
    "According to the maximum likelihood, you can decide the color $\\mu_k$ , $k \\in [1, . . . , K]$ of each pixel $x_n$ of output image\n",
    "\n",
    "#### 2.1\n",
    "\n",
    "Please build a $K$-means model by minimizing\n",
    "\n",
    "$$\n",
    "    J = \\sum_{n=1}^N \\sum_{k=1}^K \\gamma_{nk} \\| x_n - \\mu_k \\|^2\n",
    "$$\n",
    "\n",
    "and show the table of the estimated $\\{ \\mu_k \\}^K_{k=1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"./hw3.jpg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2\n",
    "\n",
    "<style>\n",
    ".blue{\n",
    "    color: skyblue;\n",
    "}\n",
    ".red{\n",
    "    color: red;\n",
    "}\n",
    ".bold{\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "Use $ \\mu = \\{ \\mu_k \\}^K_{k=1}$ calculated by the $K$-means model as the means, and calculate the corresponding variances $ \\sigma_k^2 $ \n",
    "\n",
    "and mixing coefficient $\\pi_k$ for the initialization of the GMM $p(x) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x \\ | \\ \\mu_k, \\sigma_k^2)$.\n",
    "\n",
    "Optimize the model by maximizing the log likelihood function $\\log \\ p(x \\ | \\ \\pi, \\mu, \\sigma^2)$ over N pixels through EM algorithm. \n",
    "\n",
    "<span class=\"red\">Plot the learning curve for log likelihood of GMM.</span> <span class=\"blue\">(Please terminate EM algorithm when the number of iterations arrives at 100.)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3\n",
    "\n",
    "Repeat steps 1 and 2 for $K = 2, 3, 7$ and $20$. Please show the resulting images of $K$-means model and GMM, respectively. \n",
    "\n",
    "Below are some examples.\n",
    "\n",
    "<center>\n",
    "    <img src = \"./image/figure2.png\" width = 50%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4\n",
    "\n",
    "Make some discussion about what is crucial factor to affect the output image between $K$-means and Gaussian mixture model (GMM), and explain the reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5\n",
    "\n",
    "The input image shown below comes from the licence-free dataset for personal and commercial use. \n",
    "\n",
    "Image from: https://pickupimage.com/free-photos/Cat-in-the-forest/2333003\n",
    "\n",
    "<center>\n",
    "    <img src = \"./image/figure3.png\" width = 30%>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "359ee23c21665437fbe71670f2b4cc12cabe3ec8c098faf869079ebcf8636aa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
