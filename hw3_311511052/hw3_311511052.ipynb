{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "from sklearn import svm\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "# you can choose one of the following package for image reading/processing\n",
    "\n",
    "import cv2\n",
    "import PIL\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Support Vector Machine\n",
    "\n",
    "<style>\n",
    ".blue{\n",
    "    color: skyblue;\n",
    "}\n",
    ".bold{\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "In the training procedure of SVM, we need to optimize with respect to the Lagrange multipliers\n",
    "$a = \\{ a_n \\}$. \n",
    "\n",
    "Here, we use the <span class=\"blue\">sequential minimal optimization</span> to solve the problem. \n",
    "\n",
    "For details, you can refer to the paper [Platt, John. “Sequential minimal optimization: \n",
    "\n",
    "A fast algorithm for training support vector machines”, 1998]. The classifier is written by\n",
    "\n",
    "$$\n",
    "    y(\\textbf{x}) = \\sum_{n=1}^N a_n t_n k(\\textbf{x}, \\textbf{x}_n) = \\textbf{w}^T \\textbf{x} + b\n",
    "$$\n",
    "$$\n",
    "    \\textbf{w} = \\sum_{n=1}^N \\alpha_n t_n \\phi(\\textbf{x}_n)\n",
    "$$\n",
    "$$\n",
    "    b = \\frac{1}{N_{\\mathcal{M}}} \\sum_{n \\in \\mathcal{M}} \\left( t_n - \\sum_{m \\in \\mathcal{S}} a_m t_m k(\\textbf{x}_n, \\textbf{x}_m) \\right)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{M}$ denotes the set of indices of data points having $0 \\lt a_n \\lt C$.\n",
    "\n",
    "### 1.1\n",
    "\n",
    "It is popular to use principal component analysis (PCA) to reduce the dimension of images to d = 2. \n",
    "\n",
    "Please implement it by yourself instead of using the method from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./x_train.csv\",header= None)/255\n",
    "label = pd.read_csv(\"./t_train.csv\",header= None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA**\n",
    "\n",
    "$\\textbf{C}$ is covariance matrix of $\\textbf{X}$, $\\textbf{W}$ is the eigenvector matrix of $\\textbf{C}$, $\\Lambda$ is the eigenvalue matrix of $\\textbf{C}$.\n",
    "\n",
    "$\\textbf{X}$ is the data matrix, $\\textbf{X}_k$ is the data matrix after dimension reduction, and k in this task will be 2.\n",
    "\n",
    "$$\n",
    "    \\textbf{C} = \\frac{\\textbf{X}^T \\textbf{X}}{n - 1}\n",
    "$$\n",
    "$$\n",
    "    \\textbf{C} = \\textbf{W} \\Lambda \\textbf{W}^{-1}\n",
    "$$\n",
    "$$\n",
    "    \\textbf{X}_k = \\textbf{X} \\textbf{W}_k\n",
    "$$\n",
    "\n",
    "**Using SVD to solve PCA**\n",
    "\n",
    "$\\Sigma$ is the singular value matrix of $\\textbf{X}$, $\\textbf{U}$ is the left singular vector matrix of $\\textbf{X}$, $\\textbf{V}$ is the right singular vector matrix of $\\textbf{X}$.\n",
    "\n",
    "$$\n",
    "    \\textbf{C} = \\frac{\\textbf{X}^T \\textbf{X}}{n - 1} \n",
    "$$\n",
    "$$\n",
    "    \\textbf{C} = \\frac{\\textbf{V}\\Sigma\\textbf{U}^T\\textbf{U}\\Sigma\\textbf{V}^T}{n - 1}\n",
    "$$\n",
    "$$\n",
    "    \\textbf{C} = \\textbf{V} \\frac{\\Sigma^2}{n - 1}\\textbf{V}^T\n",
    "$$\n",
    "$$\n",
    "    \\textbf{C} = \\textbf{V} \\frac{\\Sigma^2}{n - 1}\\textbf{V}^{-1} \\ \\ (\\rm{because} \\ \\textbf{V} \\ \\rm{is \\ unitary})\n",
    "$$\n",
    "\n",
    "We can found that covariance matrix $\\textbf{C}$ is the same as the eigenvalue matrix $\\Lambda$ of $\\textbf{V}^{-1}$.\n",
    "\n",
    "$$\n",
    "    \\Lambda = \\frac{\\Sigma^2}{n - 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data.values - np.mean(data.values, axis=0) # centralize data\n",
    "np.allclose(data_new.mean(axis=0), np.zeros(data_new.shape[1])) # check if centralization is correct\n",
    "C = (data_new.T @ data_new) / (data_new.shape[0] - 1) # covariance matrix\n",
    "eig_vals, eig_vecs = np.linalg.eig(C) # eigenvalues and eigenvectors\n",
    "eig_vals, eig_vecs = eig_vals.real, eig_vecs.real # convert to real numbers\n",
    "# sort eigenvalues in ascending order   \n",
    "eig_vals_sorted = np.argsort(eig_vals)[::1][-2:] # sort eigenvalues in descending order and take the first two\n",
    "data_pca = data_new @ eig_vecs[:,eig_vals_sorted] # PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 2D data with label\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(data_pca[:,0],data_pca[:,1],c=label.values)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2\n",
    "\n",
    "Describe the difference between two decision approaches (one-versus-the-rest and one-\n",
    "versus-one). \n",
    "\n",
    "Decide which one you want to choose and explain why you choose this approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".blue{\n",
    "    color: skyblue;\n",
    "}\n",
    ".red{\n",
    "    color: red;\n",
    "}\n",
    ".bold{\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "**One-versus-the-rest**\n",
    "\n",
    "First, we train a binary classifier <span class=\"red\">for each class</span>. \n",
    "\n",
    "Only one class is positive and the rest are negative.\n",
    "\n",
    "Then, we use the classifier to predict the class of the test data. \n",
    "\n",
    "Finally, we choose the class with the highest score.\n",
    "\n",
    "**One-versus-one**\n",
    "\n",
    "We train a binary classifier <span class=\"red\">for each pair of classes</sapn>. \n",
    "\n",
    "Then, we use the classifier to predict the class of the test data. \n",
    "\n",
    "The test data is assigned to the class that wins the most duels.\n",
    "\n",
    "Finally, we choose the class with the highest score.\n",
    "\n",
    "**Which one to choose**\n",
    "\n",
    "Support vector machine is a binary classifier.\n",
    "\n",
    "If we use one-versus-the-rest, we need to train $C$ binary classifiers.\n",
    "\n",
    "If we use one-versus-one, we need to train $\\frac{C(C-1)}{2}$ binary classifiers.\n",
    "\n",
    "If $C$ is large, <span class = \"red\">one-versus-one is better</sapn>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3\n",
    "\n",
    "<style>\n",
    ".blue{\n",
    "    color: skyblue;\n",
    "}\n",
    ".bold{\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "Use the principle values projected to top <span class=\"blue\">two</span> eigenvectors obtained from PCA, and build\n",
    "a SVM with <span class=\"blue\">linear kernel</span> to do multi-class classification. \n",
    "\n",
    "You can decide the upper bound $C$ of $a_n$ by yourself or just use the default value provided by sklearn. \n",
    "\n",
    "Then, <span class=\"blue\">plot the corresponding decision boundary</span> and show the <span class=\"blue\">support vectors.</span>\n",
    "\n",
    "* Linear kernel:\n",
    "$$\n",
    "    k(\\textbf{x}_i, \\textbf{x}_j) = \\phi(\\textbf{x}_i)^T \\phi(\\textbf{x}_j) = \\textbf{x}_i^T \\textbf{x}_j\n",
    "$$\n",
    "\n",
    "The sample figures are provided below.\n",
    "\n",
    "<center>\n",
    "    <img src = \"./image/figure1.png\" width = 50%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/scikit-learn/scikit-learn/blob/dc580a8ef/sklearn/svm/_classes.py#L554\n",
    "# svm with linear kernel and C=1\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "clf.fit(data_pca, label.values.ravel())\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(-0.5, 0.5)\n",
    "yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(data_pca[:,0],data_pca[:,1],c=label.values)\n",
    "plt.plot(xx, yy, 'k-')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus\n",
    "\n",
    "<style>\n",
    ".blue{\n",
    "    color: skyblue;\n",
    "}\n",
    ".bold{\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "Repeat 3 with <span class=\"blue\">polynomial kernel (degree = 2).</span>\n",
    "\n",
    "* Polynomial (homogeneous) kernel of degree 2:\n",
    "\n",
    "$$\n",
    "    k(\\textbf{x}_i, \\textbf{x}_j) = \\phi(\\textbf{x}_i)^T \\phi(\\textbf{x}_j) = \\left( \\textbf{x}_i^T \\textbf{x}_j\\right)^2\n",
    "$$\n",
    "$$\n",
    "    \\phi(\\textbf{x}) = \\left[ \\textbf{x}_1^2, \\sqrt{2} \\textbf{x}_1 \\textbf{x}_2, \\textbf{x}_2^2 \\right]\n",
    "$$\n",
    "$$\n",
    "    \\textbf{x} = \\left[ x_1, x_2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Gaussian Mixture Model\n",
    "\n",
    "<style>\n",
    ".blue{\n",
    "    color: skyblue;\n",
    "}\n",
    ".red{\n",
    "    color: red;\n",
    "}\n",
    ".bold{\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "In this exercise, you will implement a Gaussian mixture model (GMM) and apply it in image segmentation. \n",
    "\n",
    "First, use a $K$-means algorithm to find $K$ central pixels. \n",
    "\n",
    "Second, use the expectation maximization (EM) algorithm <span class=\"blue\">(please refer to textbook p.438-p.439)</span> to optimize the parameters of the model. \n",
    "\n",
    "The input image is given by <span class=\"red\">hw3.jpg</span>. \n",
    "\n",
    "According to the maximum likelihood, you can decide the color $\\mu_k$ , $k \\in [1, . . . , K]$ of each pixel $x_n$ of output image\n",
    "\n",
    "#### 2.1\n",
    "\n",
    "Please build a $K$-means model by minimizing\n",
    "\n",
    "$$\n",
    "    J = \\sum_{n=1}^N \\sum_{k=1}^K \\gamma_{nk} \\| x_n - \\mu_k \\|^2\n",
    "$$\n",
    "\n",
    "and show the table of the estimated $\\{ \\mu_k \\}^K_{k=1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2)**2))\n",
    "class KMeans():\n",
    "    def __init__(self, K=5, max_iters=100, plot_steps=False):\n",
    "        self.K = K\n",
    "        self.max_iters = max_iters\n",
    "        self.plot_steps = plot_steps\n",
    "        # list of sample indices for each cluster\n",
    "        self.clusters = [[] for _ in range(self.K)]\n",
    "        # the centers (mean feature vector) for each cluster\n",
    "        self.centroids = []\n",
    "        self.likelihood = []\n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        \n",
    "        # initialize \n",
    "        random_sample_idxs = np.random.choice(self.n_samples, self.K, replace=False)\n",
    "        self.centroids = [self.X[idx] for idx in random_sample_idxs]\n",
    "        # Optimize clusters\n",
    "        for _ in range(self.max_iters):\n",
    "            # Assign samples to closest centroids (create clusters)\n",
    "            self.clusters = self._create_clusters(self.centroids)\n",
    "            if self.plot_steps:\n",
    "                self.plot()\n",
    "            # Calculate new centroids from the clusters\n",
    "            centroids_old = self.centroids\n",
    "            self.centroids = self._get_centroids(self.clusters)\n",
    "            \n",
    "            # check if clusters have changed\n",
    "            if self._is_converged(centroids_old, self.centroids):\n",
    "                break\n",
    "            if self.plot_steps:\n",
    "                self.plot()\n",
    "        # Classify samples as the index of their clusters\n",
    "        return self._get_cluster_labels(self.clusters)\n",
    "    def _get_cluster_labels(self, clusters):\n",
    "        # each sample will get the label of the cluster it was assigned to\n",
    "        labels = np.empty(self.n_samples)\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            for sample_index in cluster:\n",
    "                labels[sample_index] = cluster_idx\n",
    "        return labels\n",
    "    def _create_clusters(self, centroids):\n",
    "        # Assign the samples to the closest centroids to create clusters\n",
    "        clusters = [[] for _ in range(self.K)]\n",
    "        for idx, sample in enumerate(self.X):\n",
    "            centroid_idx = self._closest_centroid(sample, centroids)\n",
    "            clusters[centroid_idx].append(idx)\n",
    "        return clusters\n",
    "    def _closest_centroid(self, sample, centroids):\n",
    "        # distance of the current sample to each centroid\n",
    "        distances = [euclidean_distance(sample, point) for point in centroids]\n",
    "        closest_index = np.argmin(distances)\n",
    "        return closest_index\n",
    "    def _get_centroids(self, clusters):\n",
    "        # assign mean value of clusters to centroids\n",
    "        centroids = np.zeros((self.K, self.n_features))\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            cluster_mean = np.mean(self.X[cluster], axis=0)\n",
    "            centroids[cluster_idx] = cluster_mean\n",
    "        return centroids\n",
    "    def _is_converged(self, centroids_old, centroids):\n",
    "        # distances between each old and new centroids, fol all centroids\n",
    "        distances = [euclidean_distance(centroids_old[i], centroids[i]) for i in range(self.K)]\n",
    "        return sum(distances) == 0\n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        for i, index in enumerate(self.clusters):\n",
    "            point = self.X[index].T\n",
    "            ax.scatter(*point)\n",
    "        for point in self.centroids:\n",
    "            ax.scatter(*point, marker=\"x\", color='black', linewidth=2)\n",
    "        plt.show()\n",
    "    def cent(self):\n",
    "        return self.centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"./hw3.jpg\")\n",
    "pixel_values = image.reshape((-1, 3))\n",
    "pixel_values = np.float32(pixel_values)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EM algorithm using numpy\n",
    "# def EM(X, K, max_iter=100):\n",
    "#     N, D = X.shape\n",
    "#     # initialize\n",
    "#     mu = np.random.randn(K, D)\n",
    "#     sigma = np.random.randn(K, D, D)\n",
    "#     pi = np.random.randn(K)\n",
    "#     pi = pi / np.sum(pi)\n",
    "#     for i in range(K):\n",
    "#         sigma[i] = np.eye(D)\n",
    "#     for i in range(max_iter):\n",
    "#         # E-step\n",
    "#         gamma = np.zeros((N, K))\n",
    "#         for k in range(K):\n",
    "#             gamma[:, k] = pi[k] * multivariate_normal.pdf(X, mean=mu[k], cov=sigma[k])\n",
    "#         gamma = gamma / np.sum(gamma, axis=1, keepdims=True)\n",
    "#         # M-step\n",
    "#         Nk = np.sum(gamma, axis=0)\n",
    "#         for k in range(K):\n",
    "#             mu[k] = np.sum(gamma[:, k].reshape(-1, 1) * X, axis=0) / Nk[k]\n",
    "#             sigma[k] = np.dot((gamma[:, k].reshape(-1, 1) * (X - mu[k])).T, X - mu[k]) / Nk[k]\n",
    "#         pi = Nk / N\n",
    "#     return mu, sigma, pi\n",
    "\n",
    "# # remove nan\n",
    "# pixel_values = np.nan_to_num(pixel_values)\n",
    "# pixel_values = pixel_values / 255\n",
    "\n",
    "# print(pixel_values.shape)\n",
    "\n",
    "# # postitive definite\n",
    "# for i in range(3):\n",
    "#     pixel_values[:, i] = pixel_values[:, i] + 1e-5\n",
    "\n",
    "# mu, sigma, pi = EM(pixel_values, 3)\n",
    "# print(mu)\n",
    "# print(sigma)\n",
    "# print(pi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2\n",
    "\n",
    "<style>\n",
    ".blue{\n",
    "    color: skyblue;\n",
    "}\n",
    ".red{\n",
    "    color: red;\n",
    "}\n",
    ".bold{\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "Use $ \\mu = \\{ \\mu_k \\}^K_{k=1}$ calculated by the $K$-means model as the means, and calculate the corresponding variances $ \\sigma_k^2 $ \n",
    "\n",
    "and mixing coefficient $\\pi_k$ for the initialization of the GMM $p(x) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x \\ | \\ \\mu_k, \\sigma_k^2)$.\n",
    "\n",
    "Optimize the model by maximizing the log likelihood function $\\log \\ p(x \\ | \\ \\pi, \\mu, \\sigma^2)$ over N pixels through EM algorithm. \n",
    "\n",
    "<span class=\"red\">Plot the learning curve for log likelihood of GMM.</span> <span class=\"blue\">(Please terminate EM algorithm when the number of iterations arrives at 100.)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3\n",
    "\n",
    "Repeat steps 1 and 2 for $K = 2, 3, 7$ and $20$. Please show the resulting images of $K$-means model and GMM, respectively. \n",
    "\n",
    "Below are some examples.\n",
    "\n",
    "<center>\n",
    "    <img src = \"./image/figure2.png\" width = 50%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_slot = [2, 3, 7, 20]\n",
    "for k_idx in k_slot:\n",
    "    k = KMeans(K=k_idx, max_iters=100)  \n",
    "    y_pred = k.predict(pixel_values)\n",
    "    centers = np.uint8(k.cent())\n",
    "    y_pred = y_pred.astype(int)\n",
    "    labels = y_pred.flatten()\n",
    "    segmented_image = centers[labels.flatten()]\n",
    "    segmented_image = segmented_image.reshape(image.shape)\n",
    "    plt.title(\"K = {}\".format(k_idx))\n",
    "    plt.imshow(segmented_image)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4\n",
    "\n",
    "Make some discussion about what is crucial factor to affect the output image between $K$-means and Gaussian mixture model (GMM), and explain the reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5\n",
    "\n",
    "The input image shown below comes from the licence-free dataset for personal and commercial use. \n",
    "\n",
    "Image from: https://pickupimage.com/free-photos/Cat-in-the-forest/2333003\n",
    "\n",
    "<center>\n",
    "    <img src = \"./image/figure3.png\" width = 30%>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "359ee23c21665437fbe71670f2b4cc12cabe3ec8c098faf869079ebcf8636aa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
